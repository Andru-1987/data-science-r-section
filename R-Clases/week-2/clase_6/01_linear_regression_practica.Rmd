---
title: "Regresión Lineal y Modelos Regularizados con tidymodels"
author: "Anderson Ocaña"
date: "`r Sys.Date()`"
output: 
  # html_document:
  #   toc: true
  #   toc_float: true
  #   theme: united
  #   highlight: tango
  #   code_folding: show
  pdf_document:
    toc: true
    number_sections: true
    highlight: tango
fontsize: 10pt
geometry: margin=1in
---


```{R install, include=FALSE}
packages <- c("tidymodels", "ggplot2", "dplyr")
install.packages(setdiff(packages, rownames(installed.packages())))

```

```{R setup, include=FALSE}
library(tidymodels)
library(ggplot2)
library(dplyr)
````

# 1. Introducción

En este documento trabajaremos con **regresión lineal** como modelo base y veremos variantes regularizadas (**Ridge** y **Lasso**) para mejorar la robustez y manejar multicolinealidad.
Usaremos el dataset integrado `mtcars`.

---

# 2. Carga y exploración de datos

```{R}
data(mtcars)
glimpse(mtcars)
summary(mtcars)
```

---

# 3. Definición de variables

La **variable objetivo** será `mpg` (millas por galón)
La **variable predictora** inicial: `wt` (peso del vehículo en 1000 lbs).

---

# 4. EDA rápido

```{R}
mtcars %>%
  ggplot(aes(x = wt, y = mpg)) +
  geom_point(color = "steelblue", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  labs(title = "Relación entre peso y consumo de combustible",
       x = "Peso (1000 lbs)",
       y = "Millas por galón (mpg)")
```

---

# 5. Train/Test Split

```{R}
set.seed(123)
split <- initial_split(mtcars, prop = 0.8, strata = mpg)
train_data <- training(split)
test_data  <- testing(split)
```

---

# 6. Receta de preprocesamiento

Incluimos normalización de variables predictoras.

```{R}
car_recipe <- recipe(mpg ~ wt, data = train_data) %>%
  step_normalize(all_numeric_predictors())
```

---

# 7. Modelo base: Regresión Lineal

```{R}
lm_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

lm_wf <- workflow() %>%
  add_recipe(car_recipe) %>%
  add_model(lm_model)

lm_fit <- lm_wf %>%
  fit(data = train_data)

lm_fit
```

---

# 8. Predicción y métricas en test

```{R}
lm_pred <- predict(lm_fit, test_data) %>%
  bind_cols(test_data)

metrics(lm_pred, truth = mpg, estimate = .pred)
```

---

# 9. Modelos regularizados (Ridge y Lasso)

Usaremos `glmnet` vía `parsnip`.

```{R}
ridge_model <- linear_reg(penalty = tune(), mixture = 0) %>% # Ridge
  set_engine("glmnet") %>%
  set_mode("regression")

lasso_model <- linear_reg(penalty = tune(), mixture = 1) %>% # Lasso
  set_engine("glmnet") %>%
  set_mode("regression")
```

---

# 10. Validación cruzada y tuning

```{R}
set.seed(123)
folds <- vfold_cv(train_data, v = 5)

grid <- grid_regular(penalty(range = c(-3, 1)), levels = 30)

ridge_wf <- workflow() %>% add_recipe(car_recipe) %>% add_model(ridge_model)
lasso_wf <- workflow() %>% add_recipe(car_recipe) %>% add_model(lasso_model)

ridge_res <- tune_grid(ridge_wf, resamples = folds, grid = grid, metrics = metric_set(rmse))
lasso_res <- tune_grid(lasso_wf, resamples = folds, grid = grid, metrics = metric_set(rmse))
```

---

# 11. Resultados de tuning

```{R}
autoplot(ridge_res) + ggtitle("Tuning Ridge")
autoplot(lasso_res) + ggtitle("Tuning Lasso")
```

---

# 12. Selección de hiperparámetros óptimos

```{R}
best_ridge <- select_best(ridge_res, "rmse")
best_lasso <- select_best(lasso_res, "rmse")

final_ridge <- finalize_workflow(ridge_wf, best_ridge) %>% fit(data = train_data)
final_lasso <- finalize_workflow(lasso_wf, best_lasso) %>% fit(data = train_data)
```

---

# 13. Comparativa en test set

```{R}
ridge_pred <- predict(final_ridge, test_data) %>% bind_cols(test_data)
lasso_pred <- predict(final_lasso, test_data) %>% bind_cols(test_data)

bind_rows(
  metrics(lm_pred, truth = mpg, estimate = .pred) %>% mutate(model = "Lineal"),
  metrics(ridge_pred, truth = mpg, estimate = .pred) %>% mutate(model = "Ridge"),
  metrics(lasso_pred, truth = mpg, estimate = .pred) %>% mutate(model = "Lasso")
)
```

---

# 14. Diagnóstico de residuos del modelo lineal

```{R}
augment(lm_fit$fit$fit) %>%
  ggplot(aes(.fitted, .resid)) +
  geom_point(color = "steelblue") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuos vs Ajustados (modelo lineal)",
       x = "Valores ajustados",
       y = "Residuos")
```

---

# 15. Conclusiones

* La regresión lineal es un **modelo base rápido e interpretable**.
* Ridge y Lasso permiten manejar **multicolinealidad** y mejorar generalización.
* Siempre evaluar con un set de test separado y, si es posible, con validación cruzada.
